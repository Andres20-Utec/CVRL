{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10055167,"sourceType":"datasetVersion","datasetId":6195789}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport random\nfrom torch.nn import functional as F\n\nimport torchvision.transforms.functional\nimport torch.nn as nn\nfrom torchvision import transforms\nimport torchvision.io\nfrom torch.utils.data import DataLoader, Dataset\n\n# to create the resnet layers\nclass block(nn.Module):\n\n    # identity_downsample --> conv layer\n    def __init__(self, input_channel, output_channel, identity_downsample=None, stride=1):\n        super(block,self).__init__()\n\n        # number of output channel is always 4 times the number of input channel in a block\n        self.expansion = 4\n\n        \"\"\" ---- 1st conv layer (kernel_size = 1) ---- \"\"\"\n        # 1st convolution layer\n        self.conv1 = nn.Conv3d(in_channels=input_channel,\n                               out_channels=output_channel,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0)\n\n        # 1st batch norm layer\n        self.bn1 = nn.BatchNorm3d(output_channel)\n\n        \"\"\" ---- 2nd conv layer (kernel_size = 3)---- \"\"\"\n        # 2nd convolution layer\n        # the stride is from the init\n        # the in_channels will be the output from the previous layer\n        self.conv2 = nn.Conv3d(in_channels=output_channel,\n                               out_channels=output_channel,\n                               kernel_size=3,\n                               stride=stride,\n                               padding=1)\n\n        # 2nd batch norm layer\n        self.bn2 = nn.BatchNorm3d(output_channel)\n\n        \"\"\" ---- 3rd layer conv (kernel_size = 1)---- \"\"\"\n        # 3rd convolution layer\n        # the output channel will be 4 times the number of the input channel from the previous layer\n        # the in_channels will be the output from the previous layer\n        self.conv3 = nn.Conv3d(in_channels=output_channel,\n                               out_channels=output_channel * self.expansion,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0)\n\n        # 3rd batch norm layer\n        self.bn3 = nn.BatchNorm3d(output_channel * self.expansion)\n\n        \"\"\" ---- ReLU layer ---- \"\"\"\n        self.relu = nn.ReLU()\n\n        \"\"\" ---- identity mapping ----\"\"\"\n        # conv layer that do the identity mapping\n        # to ensure same shape in the later layers\n        # If their sizes mismatch, then the input goes into an identity\n        # this is for the skipped connection\n        self.identity_downsample = identity_downsample\n\n    # forward pass\n    def forward(self, x):\n        identity = x\n\n\n        # A basic ResNet block is composed by two layers of 3x3 conv/batchnorm/relu\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        # use the identity downsample if there is a need to change the shape\n        if self.identity_downsample is not None:\n            identity = self.identity_downsample(identity)\n\n        # current output plus the residual skipped connection\n        x = x + identity\n        x = self.relu(x)\n        return x\n\n# [3,4,6,3] --> layers per block\nclass ResNet(nn.Module):\n    # block --> from the block class\n    # layers --> number of times to use the block class [3,4,6,3]\n    # image_channels --> number of channels of the input (normally is 3, RGB)\n    # num_classes --> number of classes in the data (remove it for CVLR)\n    def __init__(self, block, layers, image_channels):\n        super(ResNet,self).__init__()\n\n        # first layer\n        self.input_channel = 64\n        # kernel size is 5 as mentioned in the paper\n        # original kernel size is 7\n        self.conv1 = nn.Conv3d(in_channels=image_channels, out_channels=64, kernel_size=5, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU()\n\n        # max pooling layer\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n\n        # Resnet layers\n        # out_channels * 4 at the end\n        # all data layers are stride 2 as mentioned in the paper\n        #self.layer1 = self._make_layers(block, layers[0], out_channels=64, stride=1)\n        self.layer1 = self._make_layers(block, layers[0], out_channels=64, stride=2)\n        self.layer2 = self._make_layers(block, layers[1], out_channels=128, stride=2)\n        self.layer3 = self._make_layers(block, layers[2], out_channels=256, stride=2)\n        self.layer4 = self._make_layers(block, layers[3], out_channels=512, stride=2)\n\n        # the features\n        self.avgpool = nn.AdaptiveAvgPool3d((1,1,1))\n\n        # fc layer\n        #self.fc = nn.Linear(512 * 4, num_classes)\n\n        # MLP layer\n        # output channel * 4 from the layer 4\n        self.l1 = nn.Linear(512*4, 512*4)\n        self.l2 = nn.Linear(512*4, 128)\n\n\n    # get the features from the CNN layers\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # get the correct shape to make the output 1x1\n        \"\"\" Feature extraction \"\"\"\n        # to be use for downstream tasks\n        h = self.avgpool(x)\n\n        # reshape to send to the fully connected layer\n        #h = x.reshape(h.shape[0], -1)\n\n        \"\"\" MLP layer \"\"\"\n        h = h.squeeze()\n\n        # Projection --> (Dense --> Relu --> Dense)\n        x = self.l1(h)\n        x = self.relu(x)\n        x = self.l2(x)\n\n        # original resnet FC\n        #x = self.fc(x)\n\n        return h, x\n\n\n    # create ResNet Layers\n    # num_residual_blocks --> number of times the block class will be used\n    # out_channels --> number of channels for the output of the layer\n    # calls the block multiple times\n    def _make_layers(self, block, num_residual_blocks, out_channels, stride):\n        identity_downsample = None\n        # layers that changes the number of channels for each input channel in subsequent blocks\n        layers = []\n\n        # if the stride changes or the input channel into the nxt block changes\n        if stride != 1 or self.input_channel != out_channels * 4:\n            # only want to change the channel so the kernel size will remain as 1\n            identity_downsample = nn.Sequential(nn.Conv3d(in_channels = self.input_channel,\n                                                          out_channels=out_channels*4,\n                                                          kernel_size = 1,\n                                                          stride = stride),\n                                                nn.BatchNorm3d(out_channels * 4))\n\n        # first block --> the only changes in stride and channels\n        # out_channels will be multiplied by 4 at the end of each block\n        # identity mapping is the addition of the skipped connection and the output of the layer\n        # need to do identity downsample due to the difference in input channel in the first layer and output layer in the first block to do identity mapping\n        # it will downsample the identity via passed convolution layer to successfully perform addition\n        layers.append(block(self.input_channel, out_channels, identity_downsample, stride))\n\n        # need to change the number of input channels to match the output channels of the previous block\n        self.input_channel = out_channels * 4\n\n        # -1 because one residual block have been calculated in\n        # 'layers.append(block(self.input_channel, out_channels, identity_downsample, stride))' that changes the num of channels\n        for i in range(num_residual_blocks - 1):\n            # out_channels will be 256 after the end of the first block\n            # for this first layer, the in_channels will be 256 and the out_channels will be 64\n            # therefore, need to map 256 (in_channels) to 64 (out_channels) --> at the end of the block, 64 * 4 = 256 again\n            # stride will be one as well\n            layers.append(block(self.input_channel, out_channels))\n\n        # unpack the list of layers and pytorch will know that each layer will come after each other\n        return (nn.Sequential(*layers))\n\n\ndef ResNet_3D_50(img_channels = 3, num_classes=1000):\n    return ResNet(block, layers=[3,4,6,3], image_channels=img_channels)\n\n\ndef ResNet_3D_101(img_channels = 3, num_classes=1000):\n    return ResNet(block, layers=[3,4,23,3], image_channels=img_channels)\n\n\ndef ResNet_3D_152(img_channels = 3, num_classes=1000):\n    return ResNet(block, layers=[3,8,36,3], image_channels=img_channels)\n\ndef test():\n    model = ResNet_3D_50()\n    x = torch.randn(10, 3, 3, 224, 224)\n    # get the representations and the projections\n    ris, zis = model(x)\n    #print(y.shape)\n    print(ris.shape, zis.shape)\n\n#test()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:43.061425Z","iopub.execute_input":"2024-12-02T02:41:43.061775Z","iopub.status.idle":"2024-12-02T02:41:43.082559Z","shell.execute_reply.started":"2024-12-02T02:41:43.061742Z","shell.execute_reply":"2024-12-02T02:41:43.081644Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#!pip install av\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:43.084042Z","iopub.execute_input":"2024-12-02T02:41:43.084315Z","iopub.status.idle":"2024-12-02T02:41:43.099300Z","shell.execute_reply.started":"2024-12-02T02:41:43.084289Z","shell.execute_reply":"2024-12-02T02:41:43.098497Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"!pip install pyav","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:43.100303Z","iopub.execute_input":"2024-12-02T02:41:43.100568Z","iopub.status.idle":"2024-12-02T02:41:51.188392Z","shell.execute_reply.started":"2024-12-02T02:41:43.100543Z","shell.execute_reply":"2024-12-02T02:41:51.187462Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyav in /opt/conda/lib/python3.10/site-packages (13.1.0)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import os\n\nROOT_FOLDER = \"/kaggle/working/\"\n\nDATA_FOLDER = \"/kaggle/input/cat-vs-dog-video/data\"\n\n#DATA_LIST_FOLDER = os.path.join(ROOT_FOLDER, 'ucfTrainTestlist')\n\n#CLASS_LIST_TEXT_FILE = os.path.join(DATA_LIST_FOLDER, 'classInd.txt')\n\n#TRAIN_FOLDER_PATH = os.path.join(DATA_FOLDER, 'train')\n\n#TEST_FOLDER_PATH = os.path.join(DATA_FOLDER, 'test')\n\n#VAL_FOLDER_PATH = os.path.join(DATA_FOLDER, 'val')\n\nNUM_OF_EPOCH = 10\n\nBATCH_SIZE = 4\n\nBATCH_SIZE_TEST = 2\n\nLENGTH_OF_CLIP = 16\n\nRESIZED_FRAME = 224\n\nDATALOADER_NUM_WORKERS = 3\n\nCONTRASTIVE_LOSS_TEMP = 2\n\nSAVED_MODEL_FOLDER = \"/kaggle/working/\"\n\nSAVED_MODEL_CHECKPOINT_PATH = os.path.join(SAVED_MODEL_FOLDER, 'highest_val_acc_model.pt')\n\n#TENSORBOARD_ROOT_LOGDIR = os.path.join(ROOT_FOLDER,'tensorboard_logs')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.191069Z","iopub.execute_input":"2024-12-02T02:41:51.191477Z","iopub.status.idle":"2024-12-02T02:41:51.197472Z","shell.execute_reply.started":"2024-12-02T02:41:51.191434Z","shell.execute_reply":"2024-12-02T02:41:51.196474Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import numpy as np\n\nimport os\nimport torchvision.io\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms.functional\nimport pickle\nfrom torchvision import transforms\n\n\nimport torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.198647Z","iopub.execute_input":"2024-12-02T02:41:51.198917Z","iopub.status.idle":"2024-12-02T02:41:51.219962Z","shell.execute_reply.started":"2024-12-02T02:41:51.198892Z","shell.execute_reply":"2024-12-02T02:41:51.219227Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.220852Z","iopub.execute_input":"2024-12-02T02:41:51.221109Z","iopub.status.idle":"2024-12-02T02:41:51.231257Z","shell.execute_reply.started":"2024-12-02T02:41:51.221084Z","shell.execute_reply":"2024-12-02T02:41:51.230542Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"There are {} GPUs available\".format(torch.cuda.device_count()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.232228Z","iopub.execute_input":"2024-12-02T02:41:51.232471Z","iopub.status.idle":"2024-12-02T02:41:51.242587Z","shell.execute_reply.started":"2024-12-02T02:41:51.232448Z","shell.execute_reply":"2024-12-02T02:41:51.241765Z"}},"outputs":[{"name":"stdout","text":"There are 1 GPUs available\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"class_labeling = [['0', 'dog'], ['1', 'cat']]\nclass_name_labelling = ['dog', 'cat']\nclass_num_labelling = ['0', '1']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.244878Z","iopub.execute_input":"2024-12-02T02:41:51.245195Z","iopub.status.idle":"2024-12-02T02:41:51.257171Z","shell.execute_reply.started":"2024-12-02T02:41:51.245169Z","shell.execute_reply":"2024-12-02T02:41:51.256383Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\n# Inicialización de listas para train y test\ntrain_class, train_videos, train_num_label = [], [], []\ntest_class, test_videos, test_num_label = [], [], []\n\n# Diccionario para etiquetar clases\nclass_mapping = {\"cat\": 0, \"dog\": 1}\n\n# Función para procesar el conjunto de datos (train/test)\ndef process_data(split, class_array, videos_array, num_label_array):\n    split_dir = os.path.join(DATA_FOLDER, split)  # Ruta a train o test\n    for class_name in os.listdir(split_dir):\n        class_path = os.path.join(split_dir, class_name)\n        if os.path.isdir(class_path):  # Si es una carpeta (clase)\n            label = class_mapping[class_name]  # Obtener etiqueta de la clase\n            for video_file in os.listdir(class_path):\n                if video_file.endswith(\".mp4\"):  # Filtrar videos\n                    video_path = os.path.join(class_path, video_file)\n                    class_array.append(class_name)  # Agregar clase\n                    videos_array.append(video_path)  # Agregar ruta del video\n                    num_label_array.append(label)  # Agregar etiqueta numérica\n\n# Procesar train y test\nprocess_data(\"train\", train_class, train_videos, train_num_label)\nprocess_data(\"test\", test_class, test_videos, test_num_label)\n\n# Verificar resultados\nprint(\"Train data:\")\nprint(f\"Classes: {train_class[:5]}\")  # Muestra de las clases\nprint(f\"Videos: {train_videos[:5]}\")  # Muestra de las rutas de los videos\nprint(f\"Labels: {train_num_label[:5]}\")  # Muestra de las etiquetas\n\nprint(\"\\nTest data:\")\nprint(f\"Classes: {test_class[:5]}\")\nprint(f\"Videos: {test_videos[:5]}\")\nprint(f\"Labels: {test_num_label[:5]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.258142Z","iopub.execute_input":"2024-12-02T02:41:51.258405Z","iopub.status.idle":"2024-12-02T02:41:51.275878Z","shell.execute_reply.started":"2024-12-02T02:41:51.258359Z","shell.execute_reply":"2024-12-02T02:41:51.275036Z"}},"outputs":[{"name":"stdout","text":"Train data:\nClasses: ['dog', 'dog', 'dog', 'dog', 'dog']\nVideos: ['/kaggle/input/cat-vs-dog-video/data/train/dog/dog33.mp4', '/kaggle/input/cat-vs-dog-video/data/train/dog/dog15.mp4', '/kaggle/input/cat-vs-dog-video/data/train/dog/dog12.mp4', '/kaggle/input/cat-vs-dog-video/data/train/dog/dog19.mp4', '/kaggle/input/cat-vs-dog-video/data/train/dog/dog37.mp4']\nLabels: [1, 1, 1, 1, 1]\n\nTest data:\nClasses: ['dog', 'dog', 'dog', 'dog', 'dog']\nVideos: ['/kaggle/input/cat-vs-dog-video/data/test/dog/dog7.mp4', '/kaggle/input/cat-vs-dog-video/data/test/dog/dog1.mp4', '/kaggle/input/cat-vs-dog-video/data/test/dog/dog5.mp4', '/kaggle/input/cat-vs-dog-video/data/test/dog/dog3.mp4', '/kaggle/input/cat-vs-dog-video/data/test/dog/dog2.mp4']\nLabels: [1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import numpy as np\n\n# Listas para el conjunto de validación\nval_class = []\nval_videos = []\nval_num_label = []\n\n# Copias de las listas originales\ntrain_class_copy = train_class.copy()\ntrain_num_label_copy = train_num_label.copy()\ntrain_videos_copy = train_videos.copy()\n\n\ndef get_validation_set(set_num_per_class, original_class, original_num_label, original_videos,\n                       val_class_array, val_num_label_array, val_videos_array):\n    \"\"\"\n    Genera un conjunto de validación seleccionando una cantidad específica de elementos de cada clase.\n    \"\"\"\n    # Mapeo para contar elementos seleccionados por clase\n    class_count = {cls: 0 for cls in set(original_class)}\n\n    # Índices de elementos seleccionados para validación\n    val_indices = []\n\n    # Recorrer los datos de entrenamiento\n    for idx, cls in enumerate(original_class):\n        # Verificar si se puede agregar más elementos de esta clase al conjunto de validación\n        if class_count[cls] < set_num_per_class:\n            # Agregar datos al conjunto de validación\n            val_class_array.append(cls)\n            val_num_label_array.append(original_num_label[idx])\n            val_videos_array.append(original_videos[idx])\n            \n            # Marcar índice como seleccionado\n            val_indices.append(idx)\n            \n            # Incrementar el contador de la clase\n            class_count[cls] += 1\n\n    # Filtrar listas originales para eliminar los elementos seleccionados\n    remaining_indices = set(range(len(original_class))) - set(val_indices)\n    remaining_indices = sorted(remaining_indices)  # Asegurar orden\n\n    original_class[:] = [original_class[i] for i in remaining_indices]\n    original_num_label[:] = [original_num_label[i] for i in remaining_indices]\n    original_videos[:] = [original_videos[i] for i in remaining_indices]\n\n\n# Generar el conjunto de validación seleccionando 10 elementos por clase\nget_validation_set(\n    set_num_per_class=4,\n    original_class=train_class,\n    original_num_label=train_num_label,\n    original_videos=train_videos,\n    val_class_array=val_class,\n    val_num_label_array=val_num_label,\n    val_videos_array=val_videos\n)\n\n# Verificar resultados\nprint(\"Clases en conjunto de validación:\", val_class[:10])  # Muestra de clases\nprint(\"Videos en conjunto de validación:\", val_videos[:10])  # Muestra de videos\nprint(\"Etiquetas numéricas en validación:\", val_num_label[:10])  # Muestra de etiquetas\nprint(\"Total de elementos en validación:\", len(val_class))\n\nprint(\"Clases restantes en conjunto de entrenamiento:\", len(train_class))\nprint(\"Videos restantes en conjunto de entrenamiento:\", len(train_videos))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.277105Z","iopub.execute_input":"2024-12-02T02:41:51.277342Z","iopub.status.idle":"2024-12-02T02:41:51.286149Z","shell.execute_reply.started":"2024-12-02T02:41:51.277319Z","shell.execute_reply":"2024-12-02T02:41:51.285227Z"}},"outputs":[{"name":"stdout","text":"Clases en conjunto de validación: ['dog', 'dog', 'dog', 'dog', 'cat', 'cat', 'cat', 'cat']\nVideos en conjunto de validación: ['/kaggle/input/cat-vs-dog-video/data/train/dog/dog33.mp4', '/kaggle/input/cat-vs-dog-video/data/train/dog/dog15.mp4', '/kaggle/input/cat-vs-dog-video/data/train/dog/dog12.mp4', '/kaggle/input/cat-vs-dog-video/data/train/dog/dog19.mp4', '/kaggle/input/cat-vs-dog-video/data/train/cat/cat26.mp4', '/kaggle/input/cat-vs-dog-video/data/train/cat/cat25.mp4', '/kaggle/input/cat-vs-dog-video/data/train/cat/cat30.mp4', '/kaggle/input/cat-vs-dog-video/data/train/cat/cat9.mp4']\nEtiquetas numéricas en validación: [1, 1, 1, 1, 0, 0, 0, 0]\nTotal de elementos en validación: 8\nClases restantes en conjunto de entrenamiento: 73\nVideos restantes en conjunto de entrenamiento: 73\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"class VideoDataset(Dataset):\n\n    def __init__(self, class_labels, vid, transform = None):\n        super().__init__()\n        self.class_labels = class_labels\n        self.vid = vid\n        self.transform = transform\n\n    def __getitem__(self, index: int):\n\n        # get one video and its label\n        vid_path, class_num_label = self.vid[index], self.class_labels[index]\n\n        # can also use torch vision\n        video, audio, info = torchvision.io.read_video(filename=vid_path)\n        #print(video.size())\n\n         # Shuffle the frames randomly\n        shuffled_indices = torch.randperm(video.size()[0])\n        video = video[shuffled_indices]\n        \n        total_vid_frames = video.size()[0]\n        #print(total_vid_frames)\n\n        # random selection of 5 - 10 frames ahead\n        t = random.randint(5, 10)\n\n        # number of frames to be saved into the frame folder for each clip\n        # 16 frames to be put into the model\n        length_of_separated_clip_in_frames = LENGTH_OF_CLIP\n\n        # formula to get the sample distribution of P, which is the end point for clip 1 and, P + t for starting point of clip 2\n        # P = L-(2*W+t)\n        # allow the a few frames of overlap if clip 1 does not have enough frames for 16 frames for value p\n        p = int((total_vid_frames - (2 * length_of_separated_clip_in_frames + t))/2)\n        #print(p)\n\n        # p - 16 to get 16 frames as stated in the paper\n        # extend the clip 1 array\n        start_frame_clip_1_idx = 0\n        end_frame_clip_1_idx = 0\n\n        # if p is a value that will result in a negative frame, start from frame 0\n        if p - length_of_separated_clip_in_frames <= -1:\n            start_frame_clip_1_idx = start_frame_clip_1_idx + 0\n            end_frame_clip_1_idx = end_frame_clip_1_idx + LENGTH_OF_CLIP\n\n        else:\n            # p - 16 to get 16 frames as stated in the paper\n            start_frame_clip_1_idx = start_frame_clip_1_idx + p - length_of_separated_clip_in_frames\n\n            end_frame_clip_1_idx = end_frame_clip_1_idx + p\n\n\n\n        #print('start_frame_clip_1', start_frame_clip_1_idx)\n        #print('end_frame_clip_1', end_frame_clip_1_idx)\n\n        tensor_clip_1 = video[start_frame_clip_1_idx: end_frame_clip_1_idx]\n\n        tensor_clip_1 = torch.reshape(tensor_clip_1,\n                                      [tensor_clip_1.size()[0],\n                                       tensor_clip_1.size()[3],\n                                       tensor_clip_1.size()[1],\n                                       tensor_clip_1.size()[2]])\n        #print(len(clip_1))\n        #print('clip_1 size: ',clip_1.size())\n\n        # P + t for starting point of clip 2 as said in the paper\n        # int(p) + t + length_of_separated_clip_in_frames to get 16 frames for clip 2\n        # extend the clip 2 array\n        start_frame_clip_2_idx = p + t\n        end_frame_clip_2_idx = p + t + length_of_separated_clip_in_frames\n\n        tensor_clip_2 = video[start_frame_clip_2_idx: end_frame_clip_2_idx]\n        tensor_clip_2 = torch.reshape(tensor_clip_2,\n                                      [tensor_clip_2.size()[0],\n                                       tensor_clip_2.size()[3],\n                                       tensor_clip_2.size()[1],\n                                       tensor_clip_2.size()[2]])\n        #print(len(clip_2))\n        #print(clip_1.size())\n\n        #if len(clip_1) == len(clip_2):\n        #sample = torch.stack([clip_1, clip_2], dim=0)\n\n\n\n        if self.transform is not None:\n\n            # do transformation as PIL images on clip 1 using the TrainTransform class\n            # returns a list of transformed PIL images\n            transformed_clip_1 = self.transform(tensor_clip_1)\n\n            # do transformation as PIL images on clip 2 the TrainTransform class\n            # returns a list of transformed PIL images\n            transformed_clip_2 = self.transform(tensor_clip_2)\n\n\n            # convert the clip_1 list to tensor\n            # convert the PIL images to tensor then stack\n            tensor_clip_1 = torch.stack([transforms.functional.to_tensor(pic) for pic in transformed_clip_1])\n\n            # convert the clip_2 list to tensor\n            # convert the PIL images to tensor then stack\n            tensor_clip_2 = torch.stack([transforms.functional.to_tensor(pic) for pic in transformed_clip_2])\n\n            tensor_clip_1 = torch.reshape(tensor_clip_1,\n                                          [tensor_clip_1.size()[0],\n                                           tensor_clip_1.size()[3],\n                                           tensor_clip_1.size()[1],\n                                           tensor_clip_1.size()[2]])\n\n            tensor_clip_2 = torch.reshape(tensor_clip_2,\n                                          [tensor_clip_2.size()[0],\n                                           tensor_clip_2.size()[3],\n                                           tensor_clip_2.size()[1],\n                                           tensor_clip_2.size()[2]])\n\n\n\n            # stack by columns and return a tensor\n            #sample = torch.stack([tensor_clip_1, tensor_clip_2], dim=0)\n\n\n        # returns a tuple of clip_1, clip_2 and the its label\n        return tensor_clip_1, tensor_clip_2, class_num_label\n\n    # get the length of total dataset\n    def __len__(self):\n        return len(self.vid)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.287301Z","iopub.execute_input":"2024-12-02T02:41:51.287562Z","iopub.status.idle":"2024-12-02T02:41:51.301653Z","shell.execute_reply.started":"2024-12-02T02:41:51.287529Z","shell.execute_reply":"2024-12-02T02:41:51.300895Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\"\"\" --------- Create TrainTransform class ----------- \"\"\"\nclass CVLRTrainTransform(object):\n\n    def __init__(self):\n\n        data_transforms = [\n            transforms.RandomResizedCrop(size=RESIZED_FRAME, scale=(0.3, 1), ratio=(0.5, 2)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(brightness=0.8 * 0.3,\n                                                                               contrast=0.8 * 0.3,\n                                                                               saturation=0.8 * 0.3,\n                                                                               hue=0.8 * 0.2)]), p=0.8),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.GaussianBlur(kernel_size=1, sigma=(0.1, 2.0)),\n            #transforms.ToTensor()\n\n        ]\n        self.train_transform = transforms.Compose(data_transforms)\n\n    # sample refers to one clip\n    def __call__(self, sample):\n\n        transform = self.train_transform\n\n        transformed_clip = []\n\n        for frame in sample:\n            # takes in the frames as numpy array and convert to PIL image to do the transformation\n            #im_pil = Image.fromarray(frame)\n            im_pil = transforms.ToPILImage()(frame).convert(\"RGB\")\n            # do the transformation which will then convert to tensors\n            transf_img = transform(im_pil)\n\n            # append it to the list, which will be called by the dataset class in the '__getitem__' function\n            transformed_clip.append(transf_img)\n\n        return transformed_clip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.302576Z","iopub.execute_input":"2024-12-02T02:41:51.302822Z","iopub.status.idle":"2024-12-02T02:41:51.316234Z","shell.execute_reply.started":"2024-12-02T02:41:51.302798Z","shell.execute_reply":"2024-12-02T02:41:51.315342Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\"\"\" --------- Create TestTransform class ----------- \"\"\"\nclass CVLRTestTransform(object):\n\n    def __init__(self):\n\n        data_transforms = [\n            transforms.RandomResizedCrop(size=RESIZED_FRAME, scale=(0.3, 1), ratio=(0.5, 2)),\n            #transforms.ToTensor()\n\n        ]\n        self.train_transform = transforms.Compose(data_transforms)\n\n    # sample refers to one clip\n    def __call__(self, sample):\n\n        # call the train_transform\n        transform = self.train_transform\n\n        # get the list of transformed frames\n        transformed_clip = []\n\n        for frame in sample:\n            # takes in the frames as numpy array and convert to PIL image to do the transformation\n            #im_pil = Image.fromarray(frame)\n            im_pil = transforms.ToPILImage()(frame).convert(\"RGB\")\n            # do the transformation which will then convert to tensors\n            transf_img = transform(im_pil)\n\n            # append it to the list, which will be called by the dataset class in the '__getitem__' function\n            transformed_clip.append(transf_img)\n\n        return transformed_clip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.317277Z","iopub.execute_input":"2024-12-02T02:41:51.317511Z","iopub.status.idle":"2024-12-02T02:41:51.330826Z","shell.execute_reply.started":"2024-12-02T02:41:51.317487Z","shell.execute_reply":"2024-12-02T02:41:51.330018Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"\"\"\" ----- Train Dataloader ----- \"\"\"\ntrain_transformed_dataset = VideoDataset(class_labels=train_num_label, vid=train_videos, transform=CVLRTrainTransform())\n\ntrain_dataloader = DataLoader(train_transformed_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              # uncomment when using server\n                              # num_workers=2\n                              )\n\n\"\"\" ----- Test Dataloader ----- \"\"\"\ntest_transformed_dataset = VideoDataset(class_labels=test_num_label, vid=test_videos, transform=CVLRTestTransform())\n\ntest_dataloader = DataLoader(test_transformed_dataset,\n                              batch_size=BATCH_SIZE_TEST,\n                              shuffle=True,\n                              # uncomment when using server\n                              # num_workers=2\n                              )\n\n\n\"\"\" ----- val Dataloader ----- \"\"\"\nval_transformed_dataset = VideoDataset(class_labels=val_num_label, vid=val_videos, transform=CVLRTestTransform())\n\nval_dataloader = DataLoader(val_transformed_dataset,\n                              batch_size=BATCH_SIZE_TEST,\n                              shuffle=True,\n                              # uncomment when using server\n                              # num_workers=2\n                              )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.331910Z","iopub.execute_input":"2024-12-02T02:41:51.332178Z","iopub.status.idle":"2024-12-02T02:41:51.345433Z","shell.execute_reply.started":"2024-12-02T02:41:51.332154Z","shell.execute_reply":"2024-12-02T02:41:51.344587Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"\n\"\"\" --------- Loss function ----------- \"\"\"\n# normalized temperature-scaled cross entropy loss\n# output 1 and output 2 is the 2 different versions of the same input image\ndef nt_xent_loss(output1, output2, temperature):\n    # concatenate v1 img and v2 img via the rows, stacking vertically\n    out = torch.cat([output1, output2], dim=0)\n    n_samples = len(out)\n\n    # Full similarity matrix\n    # torch.mm --> matrix multiplication for tensors\n    # when a transposed is done on a tensor, PyTorch doesn't generate new tensor with new layout,\n    # it just modifies meta information in Tensor object so the offset and stride are for the new shape --> its memory\n    # layout is different than a tensor of same shape made from scratch\n    # contiguous --> makes a copy of tensor so the order of elements would be same as if tensor of same shape created from scratch\n    # --> https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107\n    # the diagonal of the matrix is the square of each vector element in the out vector, which shows the similarity between the same elements\n    cov = torch.mm(out, out.t().contiguous())\n    sim = torch.exp(cov/temperature)\n\n    # Negative similarity\n    # creates a 2-D tensor with True on the diagonal for the size of n_samples and False elsewhere\n    mask = ~torch.eye(n_samples, device=sim.device).bool()\n    # Returns a new 1-D tensor which indexes the input tensor (sim) according to the boolean mask (mask) which is a BoolTensor.\n    # returns a tensor with 1 row and n columns and sum it with the last dimension\n    neg = sim.masked_select(mask).view(n_samples,-1).sum(dim=-1)\n\n    # Positive similarity\n    # exp --> exponential of the sum of the last dimension after output1 * output2 divided by the temp\n    pos = torch.exp(torch.sum(output1 * output2, dim=-1)/temperature)\n    # concatenate via the rows, stacking vertically\n    pos = torch.cat([pos,pos], dim=0)\n\n    # 2 copies of the numerator as the loss is symmetric but the denominator is 2 different values --> 1 for x, 1 for y\n    # the loss will be a scalar value\n    loss = -torch.log(pos/neg).mean()\n    return loss\n\n# make a directory to save the model\nsaved_model_folder = SAVED_MODEL_FOLDER\nif not os.path.exists(saved_model_folder):\n    os.mkdir(saved_model_folder)\n\n# create a function for the 3D ResNet 50 architecture\ndef ResNet_3D_50(img_channels = 3):\n    return ResNet(block, layers=[3,4,6,3], image_channels=img_channels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.346775Z","iopub.execute_input":"2024-12-02T02:41:51.347309Z","iopub.status.idle":"2024-12-02T02:41:51.360415Z","shell.execute_reply.started":"2024-12-02T02:41:51.347266Z","shell.execute_reply":"2024-12-02T02:41:51.359533Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class CVLR(object):\n\n    def __init__(self):\n\n        #self.writer = SummaryWriter()\n        self.device = self._get_device()\n        # predefined above\n        self.nt_xent_loss = nt_xent_loss\n        self.encoder = ResNet_3D_50()\n\n    # use GPU if available\n    def _get_device(self):\n        device = torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\")\n        return device\n\n    def _step(self, model, xis, xjs, n_iter):\n        # get the representations and the projections\n        ris, zis = model(xis)  # [N,C]\n\n        # get the representations and the projections\n        rjs, zjs = model(xjs)  # [N,C]\n\n        # Verificar dimensiones\n        print(f\"Shape of zis before normalization: {zis.shape}\")\n        print(f\"Shape of zjs before normalization: {zjs.shape}\")\n    \n        # Asegurarse de que zis y zjs tengan al menos dos dimensiones\n        if zis.dim() < 2:\n            zis = zis.unsqueeze(0)\n        if zjs.dim() < 2:\n            zjs = zjs.unsqueeze(0)\n\n\n        # normalize projection feature vectors\n        zis = F.normalize(zis, dim=1)\n        zjs = F.normalize(zjs, dim=1)\n\n        loss = nt_xent_loss(zis, zjs, temperature=0.5)\n\n        return loss\n\n    def train(self):\n\n        # get the mean batch loss\n        def get_mean_of_list(L):\n\n            return sum(L) / len(L)\n\n        model = ResNet_3D_50().to(self.device)\n        model = self._load_pre_trained_weights(model)\n\n        optimizer = torch.optim.SGD(model.parameters(), lr=1.0, weight_decay=1e-6)\n\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_videos), eta_min=0,\n                                                               last_epoch=-1)\n\n        n_iter = 0\n        valid_n_iter = 0\n        best_valid_loss = np.inf\n        best_mean_batch_loss = np.inf\n\n        \"\"\"\n        batch, clip 1 and clip 2, class label\n        sample_batched\n\n        clip_1 with batch size --> torch.Size([4, 16, 3, 224, 224])\n        sample_batched[0]\n\n        clip_2 with batch size --> torch.Size([4, 16, 3, 224, 224])\n        sample_batched[1]\n\n        class label for clip 1 and clip 2 --> ('4', '1', '89', '44')\n        sample_batched[2]\n\n        clip_1 -> torch.Size([16, 3, 224, 224])\n        16 -> frames\n        3 -> colour channels\n        224 -> height of frames\n        224 -> width of frames\n        sample_batched[0][0]\n\n        clip_2 -> torch.Size([16, 3, 224, 224])\n        16 -> frames\n        3 -> colour channels\n        224 -> height of frames\n        224 -> width of frames\n        sample_batched[1][0]\n        \"\"\"\n        loss_lst = []\n        val_loss_lst = []\n        for epoch_counter in range(NUM_OF_EPOCH):\n\n            # a list to store losses for each epoch\n            epoch_losses_train = []\n\n            for i_batch, sample_batched in enumerate(train_dataloader):\n                #print(i_batch)\n                optimizer.zero_grad()\n                # print(sample_batched[1][0].size())\n\n                xis = sample_batched[0].to(device)\n                # the number of channels must be in the 2nd position else there will be an error\n                print(xis.size())\n                # xis.size()[0] -> 64 (batch size)\n                # xis.size()[3] -> 3 (colour channels)\n                # xis.size()[1] -> 16 (number of frames)\n                # xis.size()[2] -> 224 (height of frame)\n                # xis.size()[4] -> 224 (width of frame)\n                xis = torch.reshape(xis, [xis.size()[0], xis.size()[3], xis.size()[1], xis.size()[2], xis.size()[4]])\n                print(xis.size())\n\n                xjs = sample_batched[1].to(device)\n                xjs = torch.reshape(xjs, [xjs.size()[0], xjs.size()[3], xjs.size()[1], xjs.size()[2], xjs.size()[4]])\n\n                loss = self._step(model, xis, xjs, n_iter)\n\n                # put that loss value in the epoch losses list\n                epoch_losses_train.append(loss.to(self.device).data.item())\n\n                # back propagation\n                loss.backward()\n\n                optimizer.step()\n                n_iter += 1\n            # print(\"Epoch:{}\".format(epoch_counter))\n            print(\"Entrando a validation\")\n            print(\"-\" * 5)\n            valid_loss = self._validate(model, val_dataloader)\n\n            #mean of epoch losses, essentially this will reflect mean batch loss for each epoch\n            mean_batch_loss_training = get_mean_of_list(epoch_losses_train)\n            loss_lst.append(mean_batch_loss_training)\n            val_loss_lst.append(valid_loss)\n            print(\"Epoch:{} ------ Mean Batch Loss ({}) ------ Validation_loss: ({})\".format(epoch_counter, mean_batch_loss_training, valid_loss))\n            model_path = os.path.join(saved_model_folder, 'epoch_{}_model.pt'.format(epoch_counter))\n            torch.save(model.state_dict(), model_path)\n\n            \"\"\"\n            if mean_batch_loss_training < best_mean_batch_loss:\n                # save the model weights\n                best_mean_batch_loss = mean_batch_loss_training\n                torch.save(model.state_dict(), config.SAVED_MODEL_PATH_2)\n                file = os.path.join(config.MODEL_CHECKPOINT_FOLDER, 'mean_batch_loss.txt')\n                with open(file, 'w') as filetowrite:\n                    filetowrite.write(\n                        \"Epoch:{} ------ Mean Batch Loss ({}) ------ Validation_loss: ({})\".format(epoch_counter,\n                                                                                                 best_mean_batch_loss,\n                                                                                                   valid_loss))\n            \"\"\"\n\n            \"\"\"\n            if valid_loss < best_valid_loss:\n                # save the model weights\n                best_valid_loss = valid_loss\n                torch.save(model.state_dict(), config.SAVED_MODEL_PATH)\n                file = os.path.join(config.MODEL_CHECKPOINT_FOLDER, 'validation_loss.txt')\n                with open(file, 'w') as filetowrite:\n                    filetowrite.write(\n                        \"Epoch:{} ------ Mean Batch Loss ({}) ------ Validation_loss: ({})\".format(epoch_counter,\n                                                                                                   mean_batch_loss_training,\n                                                                                                   best_valid_loss))\n            \"\"\"\n\n\n            valid_n_iter += 1\n\n            # warmup for the first 10 epochs\n            if epoch_counter >= 10:\n                scheduler.step()\n        \n        return loss_lst, val_loss_lst\n\n    # validation step\n    def _validate(self, model, val_dataloader):\n        # validation steps\n        with torch.no_grad():\n            model.eval()\n\n            valid_loss = 0.0\n            counter = 0\n\n            for i_batch, sample_batched in enumerate(val_dataloader):\n                xis = sample_batched[0].to(device)\n                #print(xis.size())\n                # xis.size()[0] -> 64 (batch size)\n                # xis.size()[3] -> 3 (colour channels)\n                # xis.size()[1] -> 16 (number of frames)\n                # xis.size()[2] -> 224 (height of frame)\n                # xis.size()[4] -> 224 (width of frame)\n                xis = torch.reshape(xis, [xis.size()[0], xis.size()[3], xis.size()[1], xis.size()[2], xis.size()[4]])\n\n                xjs = sample_batched[1].to(device)\n                xjs = torch.reshape(xjs, [xjs.size()[0], xjs.size()[3], xjs.size()[1], xjs.size()[2], xjs.size()[4]])\n\n                loss = self._step(model, xis, xjs, counter)\n                valid_loss += loss.item()\n                counter += 1\n            valid_loss = valid_loss / counter\n        model.train()\n        return valid_loss\n\n\n    def _load_pre_trained_weights(self, model):\n        try:\n            state_dict = torch.load(SAVED_MODEL_CHECKPOINT_PATH)\n            model.load_state_dict(state_dict)\n            print(\"Loaded pre-trained model with success.\")\n        except FileNotFoundError:\n            print(\"Pre-trained weights not found. Training from scratch.\")\n\n        return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.361599Z","iopub.execute_input":"2024-12-02T02:41:51.361841Z","iopub.status.idle":"2024-12-02T02:41:51.384348Z","shell.execute_reply.started":"2024-12-02T02:41:51.361817Z","shell.execute_reply":"2024-12-02T02:41:51.383520Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"CVLR = CVLR()\nCVLR.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T02:41:51.385304Z","iopub.execute_input":"2024-12-02T02:41:51.385575Z"}},"outputs":[{"name":"stdout","text":"Pre-trained weights not found. Training from scratch.\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([1, 16, 224, 3, 224])\ntorch.Size([1, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([128])\nShape of zjs before normalization: torch.Size([128])\nEntrando a validation\n-----\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nEpoch:0 ------ Mean Batch Loss (1.8802494438071) ------ Validation_loss: (1.098603218793869)\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([1, 16, 224, 3, 224])\ntorch.Size([1, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([128])\nShape of zjs before normalization: torch.Size([128])\nEntrando a validation\n-----\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nEpoch:1 ------ Mean Batch Loss (1.8434850102976756) ------ Validation_loss: (1.0986114144325256)\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([1, 16, 224, 3, 224])\ntorch.Size([1, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([128])\nShape of zjs before normalization: torch.Size([128])\nEntrando a validation\n-----\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nShape of zis before normalization: torch.Size([2, 128])\nShape of zjs before normalization: torch.Size([2, 128])\nEpoch:2 ------ Mean Batch Loss (1.8434831217715621) ------ Validation_loss: (1.0986037850379944)\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\ntorch.Size([4, 16, 224, 3, 224])\ntorch.Size([4, 3, 16, 224, 224])\nShape of zis before normalization: torch.Size([4, 128])\nShape of zjs before normalization: torch.Size([4, 128])\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Inicialización de listas para train y test\ntrain_class, train_videos, train_num_label = [], [], []\ntest_class, test_videos, test_num_label = [], [], []\n\n# Diccionario para etiquetar clases\nclass_mapping = {\"cat\": 0, \"dog\": 1}\n\n# Función para procesar el conjunto de datos (train/test)\ndef process_data(split, class_array, videos_array, num_label_array):\n    split_dir = os.path.join(DATA_FOLDER, split)  # Ruta a train o test\n    for class_name in os.listdir(split_dir):\n        class_path = os.path.join(split_dir, class_name)\n        if os.path.isdir(class_path):  # Si es una carpeta (clase)\n            label = class_mapping[class_name]  # Obtener etiqueta de la clase\n            for video_file in os.listdir(class_path):\n                if video_file.endswith(\".mp4\"):  # Filtrar videos\n                    video_path = os.path.join(class_path, video_file)\n                    class_array.append(class_name)  # Agregar clase\n                    videos_array.append(video_path)  # Agregar ruta del video\n                    num_label_array.append(label)  # Agregar etiqueta numérica\n\n# Procesar train y test\nprocess_data(\"train\", train_class, train_videos, train_num_label)\nprocess_data(\"test\", test_class, test_videos, test_num_label)\n\n# Verificar resultados\nprint(\"Train data:\")\nprint(f\"Classes: {train_class[:5]}\")  # Muestra de las clases\nprint(f\"Videos: {train_videos[:5]}\")  # Muestra de las rutas de los videos\nprint(f\"Labels: {train_num_label[:5]}\")  # Muestra de las etiquetas\n\nprint(\"\\nTest data:\")\nprint(f\"Classes: {test_class[:5]}\")\nprint(f\"Videos: {test_videos[:5]}\")\nprint(f\"Labels: {test_num_label[:5]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" Load the entire video instead of the 16 frames\"\"\"\n\n\"\"\" --------- Create Dataset class ----------- \"\"\"\n\nclass VideoDataset(Dataset):\n\n    def __init__(self, class_labels, vid, transform = None):\n        super().__init__()\n        self.class_labels = class_labels\n        self.vid = vid\n        self.transform = transform\n\n    def __getitem__(self, index: int):\n\n        # get one video and its label\n        vid_path, class_num_label = self.vid[index], self.class_labels[index]\n\n        # can also use torch vision\n        video, audio, info = torchvision.io.read_video(filename=vid_path)\n        #print(video.size())\n\n        total_vid_frames = video.size()[0]\n        #print(total_vid_frames)\n\n\n\n\n        tensor_clip = torch.reshape(video,\n                                      [video.size()[0],\n                                       video.size()[3],\n                                       video.size()[1],\n                                       video.size()[2]])\n\n\n\n\n        if self.transform is not None:\n\n            # do transformation as PIL images on the entire clip using the TrainTransform class\n            # returns a list of transformed PIL images\n            transformed_clip = self.transform(tensor_clip)\n\n\n\n            # convert the entire clip list to tensor\n            # convert the PIL images to tensor then stack\n            tensor_clip = torch.stack([transforms.functional.to_tensor(pic) for pic in transformed_clip])\n\n\n            tensor_clip = torch.reshape(tensor_clip,\n                                          [tensor_clip.size()[0],\n                                           tensor_clip.size()[3],\n                                           tensor_clip.size()[1],\n                                           tensor_clip.size()[2]])\n\n\n\n\n\n        # returns a tuple of clip_1, clip_2 and the its label\n        return tensor_clip, class_num_label\n\n    # get the length of total dataset\n    def __len__(self):\n        return len(self.vid)\n\n\"\"\"\n# Test the dataset class\n\ndataset = VideoDataset(class_labels=train_num_label, vid=train_videos)\nprint(dataset.__len__())\nfirst_data = dataset[3]\n#print(first_data)\n\"\"\"\n\n\"\"\" --------- Create Transform class ----------- \"\"\"\n# only do resizing because there is no transformation needed for downstream tasks\nclass CVLRTransform(object):\n\n    def __init__(self):\n\n        data_transforms = [\n            transforms.RandomResizedCrop(size=224, scale=(0.3, 1), ratio=(0.5, 2)),\n            #transforms.ToTensor()\n\n        ]\n        self.train_transform = transforms.Compose(data_transforms)\n\n    # sample refers to one clip\n    def __call__(self, sample):\n\n        # call the train_transform\n        transform = self.train_transform\n\n        # get the list of transformed frames\n        transformed_clip = []\n\n        for frame in sample:\n            # takes in the frames as numpy array and convert to PIL image to do the transformation\n            #im_pil = Image.fromarray(frame)\n            im_pil = transforms.ToPILImage()(frame).convert(\"RGB\")\n            # do the transformation which will then convert to tensors\n            transf_img = transform(im_pil)\n\n            # append it to the list, which will be called by the dataset class in the '__getitem__' function\n            transformed_clip.append(transf_img)\n\n        return transformed_clip\n\n\n\n\"\"\" ----- Train Dataloader ----- \"\"\"\ntrain_transformed_dataset = VideoDataset(class_labels=train_num_label, vid=train_videos, transform=CVLRTransform())\n\n# batch size 1 to get the features of one video in the training set\ntrain_dataloader = DataLoader(train_transformed_dataset,\n                              batch_size=1,\n                              shuffle=True,\n                              # uncomment when using server\n                              #num_workers=3\n                              )\n\n\n\"\"\" ----- Test Dataloader ----- \"\"\"\ntest_transformed_dataset = VideoDataset(class_labels=test_num_label, vid=test_videos, transform=CVLRTransform())\n\n# batch size 1 to get the features of one video in the test set\ntest_dataloader = DataLoader(test_transformed_dataset,\n                              batch_size=1,\n                              shuffle=True,\n                              # uncomment when using server\n                              #num_workers=3\n                              )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResNet_3D_50(ResNet):\n    def __init__(self):\n        super(ResNet_3D_50, self).__init__(block, layers=[3,4,6,3], image_channels=3)\n        #self.fc = nn.Linear(512 * 4, num_classes)\n\n    # get the features from the CNN layers\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n\n        h = self.avgpool(x)\n        h = h.squeeze()\n        return h\n\n\"\"\"\n# Check the output size of the model\n\ndef test():\n    model = ResNet_3D_50()\n    x = torch.randn(10, 3, 3, 224, 224)\n    # get the representations and the projections\n    output = model(x)\n    print(output.size())\n\n#test()\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model = ResNet_3D_50()\nmodel = ResNet_3D_50()\n#model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\nmodel.eval()\nstate_dict = torch.load(\"/kaggle/working/epoch_9_model.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(state_dict)\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# freeze all the layers in the resnet model\nfor param in model.parameters():\n    param.requires_grad = False\n    #print(param)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_features_path = os.path.join(ROOT_FOLDER, 'train_features')\n\nif not os.path.exists(train_features_path):\n    os.mkdir(train_features_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train_pkl_path = os.path.join(train_features_path, 'x_train.pkl')\ny_train_pkl_path = os.path.join(train_features_path, 'y_train.pkl')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run only if the pickle file does not exist\nif os.path.isfile(x_train_pkl_path) == False or os.path.isfile(y_train_pkl_path) == False:\n\n    x_train = []\n    y_train = []\n\n    for i_batch, sample_batched in enumerate(train_dataloader):\n        img = sample_batched[0].to(device)\n        # reshape to fit into the model\n        img = torch.reshape(img, [img.size()[0], img.size()[3], img.size()[1], img.size()[2], img.size()[4]]).to(device)\n        outputs = model(img).to(device)\n        x_train.append(outputs)\n        y_train.extend(sample_batched[1])\n\n    # save x train features in a pickle file\n    with open(x_train_pkl_path, 'wb') as f:\n        pickle.dump(x_train, f)\n    # save y train labels in a pickle file\n    with open(y_train_pkl_path, 'wb') as f:\n        pickle.dump(y_train, f)\n\nelse:\n    print(\"'x_train.pkl' and 'y_train.pkl' exists.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_features_path = os.path.join(ROOT_FOLDER, 'test_features')\n\nif not os.path.exists(test_features_path):\n    os.mkdir(test_features_path)\n\nx_test_pkl_path = os.path.join(test_features_path, 'x_test.pkl')\ny_test_pkl_path = os.path.join(test_features_path, 'y_test.pkl')\n\n# run only if the pickle file does not exist\nif os.path.isfile(x_test_pkl_path) == False or os.path.isfile(y_test_pkl_path) == False:\n\n    x_test = []\n    y_test = []\n\n    for i_batch, sample_batched in enumerate(test_dataloader):\n        img = sample_batched[0].to(device)\n        # reshape to fit into the model\n        img = torch.reshape(img, [img.size()[0], img.size()[3], img.size()[1], img.size()[2], img.size()[4]]).to(device)\n        outputs = model(img).to(device)\n        x_test.append(outputs)\n        y_test.extend(sample_batched[1])\n\n    # save x test features in a pickle file\n    with open(x_test_pkl_path, 'wb') as f:\n        pickle.dump(x_test, f)\n    # save y test labels in a pickle file\n    with open(y_test_pkl_path, 'wb') as f:\n        pickle.dump(y_test, f)\n\nelse:\n    print(\"'x_test.pkl' and 'y_test.pkl' exists.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(os.path.join(train_features_path, 'x_train.pkl'), 'rb') as f:\n    x_train = pickle.load(f)\n\nwith open(os.path.join(train_features_path, 'y_train.pkl'), 'rb') as f:\n    y_train = pickle.load(f)\n\nwith open(os.path.join(test_features_path, 'x_test.pkl'), 'rb') as f:\n    x_test = pickle.load(f)\n\nwith open(os.path.join(test_features_path, 'y_test.pkl'), 'rb') as f:\n    y_test = pickle.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# need to convert tensors to numpy to load into regression model\nx_train_numpy_pkl_path = os.path.join(train_features_path, 'x_train_numpy.pkl')\n\nif os.path.isfile(x_train_numpy_pkl_path) == False:\n    x_train_numpy = []\n\n    for i in x_train:\n        i = i.detach().cpu().numpy()\n        x_train_numpy.append(i)\n\n\n    with open(x_train_numpy_pkl_path, 'wb') as f:\n        pickle.dump(x_train_numpy, f)\n\nelse:\n    print(\"'x_train_numpy.pkl' exists.\")\n\n\n# need to convert tensors to numpy to load into regression model\nx_test_numpy_pkl_path = os.path.join(test_features_path, 'x_test_numpy.pkl')\n\nif os.path.isfile(x_test_numpy_pkl_path) == False:\n\n    x_test_numpy = []\n\n    for i in x_test:\n        i = i.detach().cpu().numpy()\n        x_test_numpy.append(i)\n\n    with open(x_test_numpy_pkl_path, 'wb') as f:\n        pickle.dump(x_test_numpy, f)\n\nelse:\n    print(\"'x_test_numpy.pkl' exists.\")\n\n\nwith open(os.path.join(train_features_path, 'x_train_numpy.pkl'), 'rb') as f:\n    x_train_numpy = pickle.load(f)\n\nwith open(os.path.join(test_features_path, 'x_test_numpy.pkl'), 'rb') as f:\n    x_test_numpy = pickle.load(f)\n\n\nlogistic_regression = LogisticRegression(random_state=0, max_iter=5000, solver='lbfgs', C=1.0)\nlogistic_regression.fit(x_train_numpy, y_train)\n\ny_predict = logistic_regression.predict(x_test_numpy)\n\nacc = accuracy_score(y_test, y_predict)\nprint(\"Accuracy is {}%\".format(acc * 100))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Hello\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}